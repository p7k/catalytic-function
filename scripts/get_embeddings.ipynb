{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/stef/hiec/scripts/get_embeddings.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stef/hiec/scripts/get_embeddings.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stef/hiec/scripts/get_embeddings.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mCLEAN\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stef/hiec/scripts/get_embeddings.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mCLEAN\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m LayerNormNet\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/stef/hiec/scripts/get_embeddings.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "File \u001b[0;32m~/hiec/src/CLEAN/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39minfer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/hiec/src/CLEAN/infer.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistance_map\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevaluate\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwarn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.CLEAN.utils import *\n",
    "from src.CLEAN.model import LayerNormNet\n",
    "import os\n",
    "\n",
    "esm_embed_dir = '/home/stef/hiec/data/embeddings/esm1b/'\n",
    "\n",
    "# Redefine format esm with idx 32\n",
    "def format_esm(a):\n",
    "    if type(a) == dict:\n",
    "        a = a['mean_representations'][32]\n",
    "    return a\n",
    "\n",
    "def load_esm(fn, dir=esm_embed_dir):\n",
    "    esm = format_esm(torch.load(dir + fn))\n",
    "    return esm.unsqueeze(0) # Stick a 1 out in front of shape tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([111234, 128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get CLEAN's embeddings of Eric's precomputed ESM1b\n",
    "\n",
    "ds = 1\n",
    "esm_embeds = [load_esm(elt) for elt in os.listdir(esm_embed_dir)[::ds]] # Load Eric's esm\n",
    "fns = os.listdir(esm_embed_dir)[::ds] # Keep file names\n",
    "\n",
    "\n",
    "def get_clean_embeds(train_str, esm_embeds):\n",
    "    # Torch params\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "    \n",
    "    # load checkpoints\n",
    "    # NOTE: change this to LayerNormNet(512, 256, device, dtype) \n",
    "    # and rebuild with [python build.py install]\n",
    "    # if inferencing on model trained with supconH loss\n",
    "    model = LayerNormNet(512, 128, device, dtype)\n",
    "    checkpoint = torch.load('./data/pretrained/'+ train_str +'.pth', map_location=device) \n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "        \n",
    "    # Forward pass\n",
    "    esm_embeds = torch.cat(esm_embeds).to(device=device, dtype=dtype)\n",
    "    model_emb = model(esm_embeds)\n",
    "    return model_emb\n",
    "\n",
    "clean_embeds = get_clean_embeds(\"split100\", esm_embeds)\n",
    "clean_embeds.shape, len(fns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "\n",
    "# You must clone a slice of the tensor to\n",
    "# save data only from that slice, otherwise you\n",
    "# save the whole tensor each time because \n",
    "# actually slicing returns a view, not a new object\n",
    "\n",
    "# Detach from loss function or whetever to just \n",
    "# get the vector\n",
    "save_to = '/home/stef/hiec/data/embeddings/clean/'\n",
    "for i in range(clean_embeds.shape[0]):\n",
    "        torch.save(clean_embeds[i].clone().detach(), save_to + fns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CLEAN's own embeddings\n",
    "\n",
    "# def clean_embedding(train_data, test_data, pretrained=True, model_name=None):\n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "#     device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#     dtype = torch.float32\n",
    "#     id_ec_train, ec_id_dict_train = get_ec_id_dict('./data/' + train_data + '.csv')\n",
    "#     id_ec_test, _ = get_ec_id_dict('./data/' + test_data + '.csv')\n",
    "#     # load checkpoints\n",
    "#     # NOTE: change this to LayerNormNet(512, 256, device, dtype) \n",
    "#     # and rebuild with [python build.py install]\n",
    "#     # if inferencing on model trained with supconH loss\n",
    "#     model = LayerNormNet(512, 128, device, dtype)\n",
    "    \n",
    "#     if pretrained:\n",
    "#         try:\n",
    "#             checkpoint = torch.load('./data/pretrained/'+ train_data +'.pth', map_location=device)\n",
    "#         except FileNotFoundError as error:\n",
    "#             raise Exception('No pretrained weights for this training data')\n",
    "#     else:\n",
    "#         try:\n",
    "#             checkpoint = torch.load('./data/model/'+ model_name +'.pth', map_location=device)\n",
    "#         except FileNotFoundError as error:\n",
    "#             raise Exception('No model found!')\n",
    "        \n",
    "#     model.load_state_dict(checkpoint)\n",
    "#     model.eval()\n",
    "#     # load precomputed EC cluster center embeddings if possible\n",
    "#     if train_data == \"split70\":\n",
    "#         emb_train = torch.load('./data/pretrained/70.pt', map_location=device)\n",
    "#     elif train_data == \"split100\":\n",
    "#         emb_train = torch.load('./data/pretrained/100.pt', map_location=device)\n",
    "#     else:\n",
    "#         emb_train = model(esm_embedding(ec_id_dict_train, device, dtype))\n",
    "        \n",
    "#     emb_test = model_embedding_test(id_ec_test, model, device, dtype)\n",
    "#     return emb_test, emb_train, id_ec_train, device\n",
    "\n",
    "# emb, emb_arr, id_ec, device = clean_embedding(\"split100\", \"test\", pretrained=True)\n",
    "# print(emb.shape, emb_arr.shape, len(id_ec))\n",
    "# foo = torch.load('/home/stef/hiec/CLEAN/app/data/pretrained/100.pt', map_location=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
